import os
import json
import base64
from io import BytesIO
from dotenv import load_dotenv
from typing import List
from PIL import Image
from services.utils import clean_json_string
from schemas import DetectedObject, RippleIntent

load_dotenv()

# å°è¯•ä½¿ç”¨æ–°çš„ SDKï¼Œå¦‚æœä¸å¯ç”¨åˆ™å›é€€åˆ°æ—§çš„
try:
    from google import genai
    from google.genai import types
    USE_NEW_SDK = True
    client = genai.Client(api_key=os.getenv("GOOGLE_API_KEY"))
    print("âœ… Using new Google Genai SDK")
except ImportError:
    import google.generativeai as genai
    USE_NEW_SDK = False
    genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
    print("âš ï¸ Using old google-generativeai SDK")

# ä½¿ç”¨ä¾¿å®œå¿«é€Ÿçš„æ¨¡å‹
MODEL_NAME = 'gemini-2.0-flash'
# å›¾åƒç¼–è¾‘æ¨¡å‹ï¼ˆæ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼Œéœ€è¦ä½¿ç”¨ä¸“é—¨çš„å›¾åƒç”Ÿæˆæ¨¡å‹ï¼‰
IMAGE_EDIT_MODEL = 'gemini-2.5-flash-image'  # å®˜æ–¹æ¨èçš„å›¾åƒç¼–è¾‘æ¨¡å‹

class AIService:
    def __init__(self):
        if USE_NEW_SDK:
            self.model_name = MODEL_NAME
            self.image_edit_model_name = IMAGE_EDIT_MODEL
        else:
            self.model = genai.GenerativeModel(MODEL_NAME)
            self.image_edit_model = genai.GenerativeModel(IMAGE_EDIT_MODEL)

    async def analyze_scene(self, image) -> List[DetectedObject]:
        """
        Step 1: å…¨å±€æ‰«æ (Pre-indexing)
        è¯†åˆ«å›¾ä¸­æ‰€æœ‰ä¸»è¦ç‰©ä½“ï¼Œè¿”å›åæ ‡ã€‚
        """
        prompt = """
        Detect all significant interactable objects in this image.
        Return a JSON list. Each entry MUST follow this format:
        {
            "label": "Short object name (e.g. Vending Machine)",
            "box_2d": [ymin, xmin, ymax, xmax] (normalized 0-1000),
        }
        Focus on: furniture, appliances, people, signs, vehicles.
        Limit to top 10 most prominent objects.
        DO NOT return segmentation masks (to save tokens).
        """
        
        try:
            if USE_NEW_SDK:
                response = client.models.generate_content(
                    model=self.model_name,
                    contents=[prompt, image],
                    config=types.GenerateContentConfig(
                        temperature=0.5,
                        thinking_config=types.ThinkingConfig(thinking_budget=0)
                    )
                )
                json_str = clean_json_string(response.text)
            else:
                response = self.model.generate_content([prompt, image])
                json_str = clean_json_string(response.text)
            data = json.loads(json_str)
            
            results = []
            width, height = image.size
            
            for i, item in enumerate(data):
                # è½¬æ¢å½’ä¸€åŒ–åæ ‡åˆ°åƒç´ åæ ‡
                y0, x0, y1, x1 = item['box_2d']
                abs_box = [
                    int(y0 / 1000 * height),
                    int(x0 / 1000 * width),
                    int(y1 / 1000 * height),
                    int(x1 / 1000 * width)
                ]
                
                # è®¡ç®—ä¸­å¿ƒç‚¹
                center = ((abs_box[1] + abs_box[3]) // 2, (abs_box[0] + abs_box[2]) // 2)

                results.append(DetectedObject(
                    id=i,
                    label=item['label'],
                    box_2d=abs_box,
                    center=center
                ))
            return results
        except Exception as e:
            print(f"Analysis Error: {e}")
            return []

    async def infer_intent(self, image, clicked_label: str, nearby_labels: List[str]) -> List[RippleIntent]:
        """
        Step 2: æ„å›¾æ¨ç† (Cached Inference)
        æ ¹æ®ç‚¹å‡»çš„ç‰©ä½“ï¼Œç”Ÿæˆ Ripple Menu é€‰é¡¹ã€‚
        """
        prompt = f"""
        User clicked on a '{clicked_label}' in the image.
        Context objects nearby: {nearby_labels}.
        
        Predict 4 distinct user intents (actions) for this object.
        1. Practical (e.g., Buy, Open)
        2. Creative (e.g., Recolor, Change Style)
        3. Destructive/Edit (e.g., Remove)

        Return JSON list:
        [
            {{
                "id": 1,
                "label": "Short Button Text",
                "emoji": "Icon",
                "description": "Tooltip text",
                "color": "Hex Code (Green for Nav, Blue for Use, Orange for Edit)",
                "probability": 0.8,
                "editor_prompt": "Prompt for image generation AI to execute this"
            }}
        ]
        """
        
        try:
            if USE_NEW_SDK:
                response = client.models.generate_content(
                    model=self.model_name,
                    contents=[prompt, image],
                    config=types.GenerateContentConfig(
                        temperature=0.5,
                        thinking_config=types.ThinkingConfig(thinking_budget=0)
                    )
                )
            else:
                response = self.model.generate_content([prompt, image]) 
            
            json_str = clean_json_string(response.text)
            data = json.loads(json_str)
            
            intents = []
            for item in data:
                intents.append(RippleIntent(**item))
            return intents
        except Exception as e:
            print(f"Inference Error: {e}")
            return []

    async def execute_edit(self, image, prompt: str, box_2d: List[int], enable_image_edit: bool = True):
        """
        Step 3: æ‰§è¡Œå›¾åƒç¼–è¾‘ (Gemini Image Editing)
        ä½¿ç”¨ Gemini API è¿›è¡Œå›¾åƒç¼–è¾‘ï¼Œè¿”å›å¤„ç†åçš„å›¾ç‰‡ã€‚
        
        Args:
            image: PIL Image å¯¹è±¡
            prompt: ç¼–è¾‘æç¤ºè¯
            box_2d: ç›®æ ‡åŒºåŸŸ [y0, x0, y1, x1]
            enable_image_edit: æ˜¯å¦å¯ç”¨çœŸå®çš„å›¾åƒç¼–è¾‘ï¼ˆFalse æ—¶è¿”å›åŸå›¾ï¼‰
        """
        print(f"âš¡ï¸ Calling Gemini Image Edit with prompt: {prompt}")
        print(f"ğŸ“ Target Region: {box_2d}")
        print(f"ğŸ”„ Image Edit Enabled: {enable_image_edit}")
        
        if not enable_image_edit:
            # å¦‚æœç¦ç”¨å›¾åƒç¼–è¾‘ï¼Œè¿”å›åŸå›¾ï¼ˆç”¨äºæµ‹è¯•æˆ–æ¼”ç¤ºï¼‰
            print("âš ï¸ Image editing is disabled, returning original image")
            return image
        
        try:
            # æ„å»ºç¼–è¾‘æç¤ºè¯ï¼ŒåŒ…å«åŒºåŸŸä¿¡æ¯
            width, height = image.size
            y0, x0, y1, x1 = box_2d
            
            # å°†åæ ‡è½¬æ¢ä¸ºç›¸å¯¹ä½ç½®ï¼ˆ0-1ï¼‰
            x0_norm = x0 / width
            y0_norm = y0 / height
            x1_norm = x1 / width
            y1_norm = y1 / height
            
            # æ„å»ºåŒ…å«åŒºåŸŸä¿¡æ¯çš„å®Œæ•´æç¤ºè¯
            full_prompt = f"""Using the provided image, edit only the region at coordinates ({x0_norm:.2f}, {y0_norm:.2f}) to ({x1_norm:.2f}, {y1_norm:.2f}). 
            
{prompt}

Keep the rest of the image unchanged. Return the edited image."""
            
            # è°ƒç”¨ Gemini å›¾åƒç¼–è¾‘ API
            # æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼Œä½¿ç”¨ gemini-2.5-flash-image æ¨¡å‹è¿›è¡Œå›¾åƒç¼–è¾‘
            # å®˜æ–¹ç¤ºä¾‹ä½¿ç”¨ [text_input, image_input] æˆ– [image_input, text_input]
            # ä½¿ç”¨ response_modalities=['Image'] ç¡®ä¿åªè¿”å›å›¾ç‰‡ï¼Œä¸è¿”å›æ–‡æœ¬
            if USE_NEW_SDK:
                response = client.models.generate_content(
                    model=self.image_edit_model_name,
                    contents=[full_prompt, image],  # æŒ‰ç…§å®˜æ–¹ç¤ºä¾‹ï¼šæ–‡æœ¬åœ¨å‰ï¼Œå›¾ç‰‡åœ¨å
                    config=types.GenerateContentConfig(
                        response_modalities=['Image'],  # åªè¿”å›å›¾ç‰‡ï¼Œä¸è¿”å›æ–‡æœ¬
                        # image_config=types.ImageConfig(
                        #     aspect_ratio="16:9",  # å¯é€‰ï¼šæ§åˆ¶è¾“å‡ºå›¾ç‰‡çš„æ˜¾ç¤ºæ¯”ä¾‹
                        # ),
                    )
                )
            else:
                # æ—§ SDK ä¹Ÿå°è¯•ä½¿ç”¨ç›¸åŒçš„é¡ºåº
                response = self.image_edit_model.generate_content([full_prompt, image])
            
            # æ£€æŸ¥å“åº”æ˜¯å¦æœ‰æ•ˆ
            if not response:
                print("âš ï¸ Empty response from API")
                return image
            
            # æ£€æŸ¥ candidatesï¼ˆå“åº”å¯èƒ½è¢«å®‰å…¨ç­–ç•¥é˜»æ­¢ï¼‰
            if hasattr(response, 'candidates'):
                if not response.candidates or len(response.candidates) == 0:
                    print("âš ï¸ No candidates in response (may be blocked by safety settings)")
                    # æ£€æŸ¥æ˜¯å¦æœ‰é˜»æ­¢åŸå› 
                    if hasattr(response, 'prompt_feedback'):
                        print(f"ğŸ“‹ Prompt feedback: {response.prompt_feedback}")
                    return image
            
            # ä»å“åº”ä¸­æå–å›¾ç‰‡ï¼ˆæŒ‰ç…§å®˜æ–¹æ–‡æ¡£çš„æ–¹å¼ï¼‰
            try:
                # å®˜æ–¹æ–‡æ¡£ç¤ºä¾‹ï¼šéå† response.parts
                if hasattr(response, 'parts'):
                    for part in response.parts:
                        # æ–¹æ³•1: æ£€æŸ¥æ–‡æœ¬å“åº”
                        if hasattr(part, 'text') and part.text is not None:
                            print(f"ğŸ“ Response text: {part.text}")
                        
                        # æ–¹æ³•2: æ£€æŸ¥ inline_data å¹¶ä½¿ç”¨ as_image()ï¼ˆå®˜æ–¹æ¨èæ–¹å¼ï¼‰
                        # æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼šhttps://ai.google.dev/gemini-api/docs/image-generation
                        # part.as_image() è¿”å›çš„å¯¹è±¡å¯ä»¥ç›´æ¥è°ƒç”¨ save() æ–¹æ³•
                        elif hasattr(part, 'inline_data') and part.inline_data is not None:
                            # å…ˆæ‰“å° inline_data çš„ç»“æ„ç”¨äºè°ƒè¯•
                            print(f"ğŸ“‹ inline_data type: {type(part.inline_data)}")
                            print(f"ğŸ“‹ inline_data attributes: {[attr for attr in dir(part.inline_data) if not attr.startswith('_')]}")
                            
                            try:
                                # æ–¹æ³•1: å°è¯•ç›´æ¥ä½¿ç”¨ as_image()ï¼ˆå®˜æ–¹æ¨èï¼‰
                                edited_image = part.as_image()
                                print(f"ğŸ“‹ as_image() returned type: {type(edited_image)}")
                                if edited_image:
                                    # æ£€æŸ¥æ˜¯å¦æ˜¯ PIL Image
                                    if isinstance(edited_image, Image.Image):
                                        if edited_image.mode != 'RGB':
                                            edited_image = edited_image.convert('RGB')
                                        print("âœ… Image editing successful (from part.as_image() - PIL Image)")
                                        return edited_image
                                    else:
                                        print(f"âš ï¸ as_image() returned non-PIL type: {type(edited_image)}")
                                        # å¦‚æœä¸æ˜¯ PIL Imageï¼Œå°è¯•å…¶ä»–æ–¹æ³•
                                        raise ValueError(f"as_image() returned non-PIL type: {type(edited_image)}")
                            except Exception as e:
                                print(f"âš ï¸ Error using as_image(): {e}")
                                
                            # æ–¹æ³•2: å°è¯•ä» inline_data æ‰‹åŠ¨è§£ç 
                            try:
                                # æ£€æŸ¥ä¸åŒçš„æ•°æ®è®¿é—®æ–¹å¼
                                image_data = None
                                
                                # æ–¹å¼1: ç›´æ¥è®¿é—® data å±æ€§
                                if hasattr(part.inline_data, 'data'):
                                    data_attr = part.inline_data.data
                                    print(f"ğŸ“‹ inline_data.data type: {type(data_attr)}")
                                    if isinstance(data_attr, str):
                                        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯• base64 è§£ç 
                                        image_data = base64.b64decode(data_attr)
                                    elif isinstance(data_attr, bytes):
                                        # å¦‚æœå·²ç»æ˜¯ bytesï¼Œç›´æ¥ä½¿ç”¨
                                        image_data = data_attr
                                    else:
                                        # å°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²å†è§£ç 
                                        data_str = str(data_attr)
                                        if data_str.startswith('data:'):
                                            # å¤„ç† data URI æ ¼å¼
                                            data_str = data_str.split('base64,')[1] if 'base64,' in data_str else data_str
                                        image_data = base64.b64decode(data_str)
                                
                                # æ–¹å¼2: å°è¯•è®¿é—® bytes å±æ€§
                                elif hasattr(part.inline_data, 'bytes'):
                                    image_data = part.inline_data.bytes
                                
                                # æ–¹å¼3: å°è¯•è®¿é—® raw_data å±æ€§
                                elif hasattr(part.inline_data, 'raw_data'):
                                    raw = part.inline_data.raw_data
                                    if isinstance(raw, bytes):
                                        image_data = raw
                                    elif isinstance(raw, str):
                                        image_data = base64.b64decode(raw)
                                
                                if image_data:
                                    # å°è¯•æ‰“å¼€å›¾ç‰‡
                                    edited_image = Image.open(BytesIO(image_data))
                                    if edited_image.mode != 'RGB':
                                        edited_image = edited_image.convert('RGB')
                                    print("âœ… Image editing successful (from inline_data manual decode)")
                                    return edited_image
                                else:
                                    raise ValueError("Could not extract image data from inline_data")
                                    
                            except Exception as e2:
                                print(f"âš ï¸ Error in manual decoding: {e2}")
                                import traceback as tb
                                tb.print_exc()
                
                # æ–° SDK å¯èƒ½è¿˜æœ‰å…¶ä»–æ–¹å¼è®¿é—®å›¾ç‰‡
                # æ³¨æ„ï¼šæ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼Œåº”è¯¥ä½¿ç”¨ response.partsï¼Œè€Œä¸æ˜¯ response.images
                # æ‰€ä»¥è¿™é‡Œæš‚æ—¶æ³¨é‡Šæ‰ï¼Œä¼˜å…ˆä½¿ç”¨ä¸Šé¢çš„ parts éå†æ–¹å¼
                # if USE_NEW_SDK:
                #     if hasattr(response, 'images') and response.images:
                #         edited_image = response.images[0]
                #         print("âœ… Image editing successful (from response.images)")
                #         return edited_image
                
                # æ—§ SDK çš„ candidates æ–¹å¼ï¼ˆå‘åå…¼å®¹ï¼‰
                if hasattr(response, 'candidates') and response.candidates:
                    candidate = response.candidates[0]
                    if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                        for part in candidate.content.parts:
                            if hasattr(part, 'inline_data') and part.inline_data:
                                try:
                                    # ä¼˜å…ˆæ‰‹åŠ¨è§£ç  base64
                                    image_data = base64.b64decode(part.inline_data.data)
                                    edited_image = Image.open(BytesIO(image_data))
                                    if edited_image.mode != 'RGB':
                                        edited_image = edited_image.convert('RGB')
                                    print("âœ… Image editing successful (from candidate inline_data)")
                                    return edited_image
                                except Exception as e:
                                    print(f"âš ï¸ Error decoding candidate inline_data: {e}")
                                    # å›é€€åˆ° as_image()
                                    try:
                                        edited_image = part.as_image()
                                        if edited_image:
                                            # å¦‚æœæ˜¯ google.genai.types.Imageï¼Œéœ€è¦è½¬æ¢
                                            if not isinstance(edited_image, Image.Image):
                                                # å°è¯•ä» inline_data è·å–æ•°æ®
                                                if hasattr(part, 'inline_data') and part.inline_data:
                                                    image_data = base64.b64decode(part.inline_data.data)
                                                    edited_image = Image.open(BytesIO(image_data))
                                            if edited_image.mode != 'RGB':
                                                edited_image = edited_image.convert('RGB')
                                            print("âœ… Image editing successful (from candidate.parts.as_image())")
                                            return edited_image
                                    except Exception as e2:
                                        print(f"âš ï¸ Error using candidate as_image(): {e2}")
                    
            except Exception as e:
                print(f"âš ï¸ Error parsing response: {e}")
                import traceback
                traceback.print_exc()
            
            # å¦‚æœæ²¡æœ‰è¿”å›å›¾ç‰‡ï¼Œè¿”å›åŸå›¾
            print("âš ï¸ No image data in response, returning original image")
            print(f"ğŸ“‹ Response type: {type(response)}")
            if hasattr(response, 'text'):
                print(f"ğŸ“‹ Response text (first 200 chars): {response.text[:200]}")
            if hasattr(response, 'parts'):
                print(f"ğŸ“‹ Response has {len(response.parts)} parts")
                for i, part in enumerate(response.parts):
                    print(f"ğŸ“‹ Part {i} attributes: {[attr for attr in dir(part) if not attr.startswith('_')]}")
            return image
            
        except Exception as e:
            print(f"âŒ Image editing error: {e}")
            # å‡ºé”™æ—¶è¿”å›åŸå›¾
            return image

